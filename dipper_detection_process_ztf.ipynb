{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all of the dippers in the ZTF dataset\n",
    "\n",
    "This notebook processes the full ZTF dataset to look for dips and creates a new AXS dataset with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import axs\n",
    "import pyspark.sql.functions as sparkfunc\n",
    "\n",
    "import dipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_start(local_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"LSD2\")\n",
    "            .config(\"spark.sql.warehouse.dir\", local_dir)\n",
    "            .config('spark.master', \"local[20]\")\n",
    "            .config('spark.driver.memory', '8G') # 128\n",
    "            .config('spark.local.dir', local_dir)\n",
    "            .config('spark.memory.offHeap.enabled', 'true')\n",
    "            .config('spark.memory.offHeap.size', '4G') # 256\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"6G\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home={local_dir}\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "    )   \n",
    "\n",
    "    return spark\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark_session = spark_start(f\"/epyc/users/{username}/spark-tmp/\")\n",
    "\n",
    "catalog = axs.AxsCatalog(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the full ZTF dataset\n",
    "\n",
    "Warning! This takes a long time to run. With the latest dipper code, it is ~300 core hours. It is recommended to increase the number of cores that spark is running on before you execute this (e.g, in the spark setup change `.config('spark.master', \"local[6]\")` which uses 6 cores to something larger. It is also possible to run on AWS with hundreds or even thousands of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full ZTF dataset\n",
    "ztf = catalog.load('ztf_oct19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run the dip detection query on the full ZTF dataset on spark\n",
    "result = (\n",
    "    ztf\n",
    "    .exclude_duplicates()\n",
    "    #.drop('dip')\n",
    "    .where(\n",
    "        (ztf[\"nobs_g\"] >= 10)\n",
    "        | (ztf[\"nobs_r\"] >= 10)\n",
    "        | (ztf[\"nobs_i\"] >= 10)\n",
    "    )\n",
    "    .select(\n",
    "        '*',\n",
    "        dipper.build_measure_dip_udf()(\n",
    "            ztf['mjd_g'],\n",
    "            ztf['mag_g'],\n",
    "            ztf['magerr_g'],\n",
    "            ztf['xpos_g'],\n",
    "            ztf['ypos_g'],\n",
    "            ztf['catflags_g'],\n",
    "            ztf['mjd_r'],\n",
    "            ztf['mag_r'],\n",
    "            ztf['magerr_r'],\n",
    "            ztf['xpos_r'],\n",
    "            ztf['ypos_r'],\n",
    "            ztf['catflags_r'],\n",
    "            ztf['mjd_i'],\n",
    "            ztf['mag_i'],\n",
    "            ztf['magerr_i'],\n",
    "            ztf['xpos_i'],\n",
    "            ztf['ypos_i'],\n",
    "            ztf['catflags_i']\n",
    "        ).alias('dip'),\n",
    "    )\n",
    "    .where(\n",
    "        (sparkfunc.col(\"dip.significance\") >= 5)\n",
    "    )\n",
    ")\n",
    "\n",
    "catalog.save_axs_table(result, 'wtf_integral_full_2', repartition=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-AXS Spark",
   "language": "python",
   "name": "spark-smj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
