{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all of the dippers in the ZTF dataset\n",
    "\n",
    "This notebook processes the full ZTF dataset to look for dips and creates a new AXS dataset with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import axs\n",
    "import pyspark.sql.functions as sparkfunc\n",
    "\n",
    "import dipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_start(local_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"LSD2\")\n",
    "            .config(\"spark.sql.warehouse.dir\", local_dir)\n",
    "            .config('spark.master', \"local[20]\")\n",
    "            .config('spark.driver.memory', '8G') # 128\n",
    "            .config('spark.local.dir', local_dir)\n",
    "            .config('spark.memory.offHeap.enabled', 'true')\n",
    "            .config('spark.memory.offHeap.size', '4G') # 256\n",
    "            .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"6G\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home={local_dir}\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "    )   \n",
    "\n",
    "    return spark\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark_session = spark_start(f\"/epyc/users/{username}/spark-tmp/\")\n",
    "\n",
    "catalog = axs.AxsCatalog(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://epyc.astro.washington.edu:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-SNAPSHOT</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[20]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LSD2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd1f43dc790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the full ZTF dataset\n",
    "\n",
    "Warning! This takes a long time to run. With the latest dipper code, it is ~300 core hours. It is recommended to increase the number of cores that spark is running on before you execute this (e.g, in the spark setup change `.config('spark.master', \"local[6]\")` which uses 6 cores to something larger. It is also possible to run on AWS with hundreds or even thousands of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full ZTF dataset\n",
    "ztf = catalog.load('ztf_march2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 2.72 s, total: 10.5 s\n",
      "Wall time: 16h 12min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run the dip detection query on the full ZTF dataset on spark\n",
    "result = (\n",
    "    ztf\n",
    "    #.exclude_duplicates()\n",
    "    #.drop('dip')\n",
    "    .where(\n",
    "        (ztf[\"nobs_g\"] >= 10)\n",
    "        | (ztf[\"nobs_r\"] >= 10)\n",
    "        | (ztf[\"nobs_i\"] >= 10)\n",
    "    )\n",
    "    .select(\n",
    "        '*',\n",
    "        dipper.build_measure_dip_udf()(\n",
    "            ztf['mjd_g'],\n",
    "            ztf['mag_g'],\n",
    "            ztf['magerr_g'],\n",
    "            ztf['xpos_g'],\n",
    "            ztf['ypos_g'],\n",
    "            ztf['catflags_g'],\n",
    "            ztf['mjd_r'],\n",
    "            ztf['mag_r'],\n",
    "            ztf['magerr_r'],\n",
    "            ztf['xpos_r'],\n",
    "            ztf['ypos_r'],\n",
    "            ztf['catflags_r'],\n",
    "            ztf['mjd_i'],\n",
    "            ztf['mag_i'],\n",
    "            ztf['magerr_i'],\n",
    "            ztf['xpos_i'],\n",
    "            ztf['ypos_i'],\n",
    "            ztf['catflags_i']\n",
    "        ).alias('dip'),\n",
    "    )\n",
    "    .where(\n",
    "        (sparkfunc.col(\"dip.significance\") >= 5)\n",
    "    )\n",
    ")\n",
    "\n",
    "catalog.save_axs_table(result, 'wtf_march2020_full_2', repartition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo the dipper calculations on a subset of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a catalog where we have used loose selection cuts\n",
    "# to find dippers. This has 4.5M lightcurves\n",
    "wtf = catalog.load('wtf_integral_full_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the dipper scores. Note, Spark uses lazy evaluation, so\n",
    "# this doesn't actually do anything until we do something with\n",
    "# the results.\n",
    "result = (\n",
    "    wtf\n",
    "    .drop('dip')\n",
    "    .select(\n",
    "        '*',\n",
    "        dipper.build_measure_dip_udf()(\n",
    "            wtf['mjd_g'],\n",
    "            wtf['mag_g'],\n",
    "            wtf['magerr_g'],\n",
    "            wtf['xpos_g'],\n",
    "            wtf['ypos_g'],\n",
    "            wtf['catflags_g'],\n",
    "            wtf['mjd_r'],\n",
    "            wtf['mag_r'],\n",
    "            wtf['magerr_r'],\n",
    "            wtf['xpos_r'],\n",
    "            wtf['ypos_r'],\n",
    "            wtf['catflags_r'],\n",
    "            wtf['mjd_i'],\n",
    "            wtf['mag_i'],\n",
    "            wtf['magerr_i'],\n",
    "            wtf['xpos_i'],\n",
    "            wtf['ypos_i'],\n",
    "            wtf['catflags_i'],\n",
    "        ).alias('dip'),\n",
    "    )\n",
    "    .where(\n",
    "        (sparkfunc.col(\"dip.significance\") >= 5)\n",
    "    )\n",
    ")\n",
    "catalog.save_axs_table(result, 'wtf_integral_full_8', repartition=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-AXS Spark",
   "language": "python",
   "name": "spark-smj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
